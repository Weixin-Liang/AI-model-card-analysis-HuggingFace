---
language: ko
license: mit
tags:
- bart
---


# Model Card for kobart-base-v2 

 
# Model Details
 
## Model Description
 
[**BART**](https://arxiv.org/pdf/1910.13461.pdf)(**B**idirectional and **A**uto-**R**egressive **T**ransformers)ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ ì¼ë¶€ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì´ë¥¼ ë‹¤ì‹œ ì›ë¬¸ìœ¼ë¡œ ë³µêµ¬í•˜ëŠ” `autoencoder`ì˜ í˜•íƒœë¡œ í•™ìŠµì´ ë©ë‹ˆë‹¤. í•œêµ­ì–´ BART(ì´í•˜ **KoBART**) ëŠ” ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ `Text Infilling` ë…¸ì´ì¦ˆ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ **40GB** ì´ìƒì˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œ í•™ìŠµí•œ í•œêµ­ì–´ `encoder-decoder` ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë„ì¶œëœ `KoBART-base`ë¥¼ ë°°í¬í•©ë‹ˆë‹¤.

- **Developed by:** More information needed
- **Shared by [Optional]:** Heewon(Haven) Jeon
- **Model type:** Feature Extraction 
- **Language(s) (NLP):** Korean
- **License:** MIT
- **Parent Model:** BART
- **Resources for more information:**
  - [GitHub Repo](https://github.com/haven-jeon/KoBART)
   - [Model Demo Space](https://huggingface.co/spaces/gogamza/kobart-summarization)
 	


# Uses
 

## Direct Use
This model can be used for the task of Feature Extraction.
 
## Downstream Use [Optional]
 
More information needed.
 
## Out-of-Scope Use
 
The model should not be used to intentionally create hostile or alienating environments for people. 
 
# Bias, Risks, and Limitations
 
 
Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.



## Recommendations
 
 
Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

# Training Details
 
## Training Data
 
| Data  | # of Sentences |
|-------|---------------:|
| Korean Wiki |     5M   |  
| Other corpus |  0.27B    | 
 
í•œêµ­ì–´ ìœ„í‚¤ ë°±ê³¼ ì´ì™¸, ë‰´ìŠ¤, ì±…, [ëª¨ë‘ì˜ ë§ë­‰ì¹˜ v1.0(ëŒ€í™”, ë‰´ìŠ¤, ...)](https://corpus.korean.go.kr/), [ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì›](https://github.com/akngs/petitions) ë“±ì˜ ë‹¤ì–‘í•œ ë°ì´í„°ê°€ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.
 
`vocab` ì‚¬ì´ì¦ˆëŠ” 30,000 ì´ë©° ëŒ€í™”ì— ìì£¼ ì“°ì´ëŠ” ì•„ë˜ì™€ ê°™ì€ ì´ëª¨í‹°ì½˜, ì´ëª¨ì§€ ë“±ì„ ì¶”ê°€í•˜ì—¬ í•´ë‹¹ í† í°ì˜ ì¸ì‹ ëŠ¥ë ¥ì„ ì˜¬ë ¸ìŠµë‹ˆë‹¤. 
> ğŸ˜€, ğŸ˜, ğŸ˜†, ğŸ˜…, ğŸ¤£, .. , `:-)`, `:)`, `-)`, `(-:`...
 
## Training Procedure

 
### Tokenizer
 
[`tokenizers`](https://github.com/huggingface/tokenizers) íŒ¨í‚¤ì§€ì˜ `Character BPE tokenizer`ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. 
 
 


 
### Speeds, Sizes, Times
| Model       |  # of params |   Type   | # of layers  | # of heads | ffn_dim | hidden_dims | 
|--------------|:----:|:-------:|--------:|--------:|--------:|--------------:|
| `KoBART-base` |  124M  |  Encoder |   6     | 16      | 3072    | 768 | 
|               |        | Decoder |   6     | 16      | 3072    | 768 |

 
# Evaluation
 
 
## Testing Data, Factors & Metrics
 
### Testing Data
 
More information needed 
 
 
### Factors
More information needed
 
### Metrics
 
More information needed
 
 
## Results 
 
NSMC
- acc. : 0.901

The model authors also note in the [GitHub Repo](https://github.com/haven-jeon/KoBART):

|   |  [NSMC](https://github.com/e9t/nsmc)(acc)  | [KorSTS](https://github.com/kakaobrain/KorNLUDatasets)(spearman) | [Question Pair](https://github.com/aisolab/nlp_classification/tree/master/BERT_pairwise_text_classification/qpair)(acc) |
|---|---|---|---|
| **KoBART-base**  | 90.24  | 81.66  | 94.34  |
 
# Model Examination
 
More information needed
 
# Environmental Impact
 
Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).
 
- **Hardware Type:** More information needed
- **Hours used:** More information needed
- **Cloud Provider:** More information needed
- **Compute Region:** More information needed
- **Carbon Emitted:** More information needed
 
# Technical Specifications [optional]
 
## Model Architecture and Objective

More information needed 
 
## Compute Infrastructure
 
More information needed 
 
### Hardware
 
 
More information needed
 
### Software
 
More information needed.
 
# Citation

 
**BibTeX:**
 
 
More information needed.

 
 
 
 
# Glossary [optional]
More information needed 
 
# More Information [optional]
More information needed 

 
# Model Card Authors [optional]
 
 Heewon(Haven) Jeon in collaboration with Ezi Ozoani and the Hugging Face team


# Model Card Contact
 The model authors note in the [GitHub Repo](https://github.com/haven-jeon/KoBART):
`KoBART` ê´€ë ¨ ì´ìŠˆëŠ” [ì´ê³³](https://github.com/SKT-AI/KoBART/issues)ì— ì˜¬ë ¤ì£¼ì„¸ìš”.
 
# How to Get Started with the Model
 
Use the code below to get started with the model.
 
<details>
<summary> Click to expand </summary>

```python
 from transformers import PreTrainedTokenizerFast, BartModel

tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')
model = BartModel.from_pretrained('gogamza/kobart-base-v2')
 ```
</details>
